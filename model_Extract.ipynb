{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "model_Extract",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliikhwan99/LLM_MODEL/blob/main/model_Extract.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call Mallam model"
      ],
      "metadata": {
        "id": "E4eUQm672A8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T03:07:43.395319Z",
          "iopub.execute_input": "2025-04-20T03:07:43.395585Z",
          "iopub.status.idle": "2025-04-20T03:07:59.790543Z",
          "shell.execute_reply.started": "2025-04-20T03:07:43.395559Z",
          "shell.execute_reply": "2025-04-20T03:07:59.789877Z"
        },
        "id": "5ayD7zpZ2A8V",
        "outputId": "2db13733-3864-441f-ae70-5b7ca7acc7f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nCollecting transformers\n  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.1\n    Uninstalling transformers-4.51.1:\n      Successfully uninstalled transformers-4.51.1\nSuccessfully installed transformers-4.51.3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Create pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"mesolitica/mallam-1.1B-4096\")\n",
        "\n",
        "# Generate text\n",
        "output = pipe(\"Apa khabar?\", max_length=100, do_sample=True)\n",
        "print(output[0]['generated_text'])\n",
        "\n",
        "# Alternatively, use model and tokenizer directly\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"mesolitica/mallam-1.1B-4096\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mesolitica/mallam-1.1B-4096\")\n",
        "\n",
        "inputs = tokenizer(\"Apa khabar?\", return_tensors=\"pt\")\n",
        "outputs = model.generate(**inputs, max_length=100, do_sample=True)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T03:07:59.792416Z",
          "iopub.execute_input": "2025-04-20T03:07:59.792643Z",
          "iopub.status.idle": "2025-04-20T03:09:04.595053Z",
          "shell.execute_reply.started": "2025-04-20T03:07:59.792623Z",
          "shell.execute_reply": "2025-04-20T03:09:04.594245Z"
        },
        "id": "ia5DEqUk2A8X",
        "outputId": "ef628d62-ec02-4606-ef39-5ffb3c1d5a49",
        "colab": {
          "referenced_widgets": [
            "9a9f69e368614156aaa1301b90a32740",
            "ba76a6edc8e14d3db15f084fcde3f41b",
            "1dbb3e8c0bd9451580def62373ffa236",
            "c64f41d6d7924afa829a59ccf2b15100",
            "4f768233bb294731a2875dfc17903682",
            "2eaa70e39ac945b19af9c15c8cccfdaa"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-20 03:08:15.658218: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745118496.101307      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745118496.222438      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/668 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a9f69e368614156aaa1301b90a32740"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/2.25G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba76a6edc8e14d3db15f084fcde3f41b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dbb3e8c0bd9451580def62373ffa236"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c64f41d6d7924afa829a59ccf2b15100"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f768233bb294731a2875dfc17903682"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/638 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2eaa70e39ac945b19af9c15c8cccfdaa"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Apa khabar?\\nAnda ada soalan nak tanya? Boleh terus tanya di sini. Mana tahu kalau saya boleh bantu menjawab dan kita kongsi ilmu bersama.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Apa khabar? Bagaimana rasa madu? Bagaimana rasanya?\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation for translation task"
      ],
      "metadata": {
        "id": "i6r6GO6x2A8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Prepare a Translation Dataset"
      ],
      "metadata": {
        "id": "fzPEEoet2A8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [\n",
        "    {\"input\": \"Hello, how are you?\", \"expected\": \"Hai, apa khabar?\"},\n",
        "    {\"input\": \"I love to eat nasi lemak.\", \"expected\": \"Saya suka makan nasi lemak.\"},\n",
        "    {\"input\": \"Where is the nearest hospital?\", \"expected\": \"Di manakah hospital yang terdekat?\"},\n",
        "]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T03:09:04.595877Z",
          "iopub.execute_input": "2025-04-20T03:09:04.596422Z",
          "iopub.status.idle": "2025-04-20T03:09:04.600225Z",
          "shell.execute_reply.started": "2025-04-20T03:09:04.596402Z",
          "shell.execute_reply": "2025-04-20T03:09:04.599521Z"
        },
        "id": "Aj8Vmjex2A8Y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Prompt the Model to Translate"
      ],
      "metadata": {
        "id": "SfTtVj8I2A8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"mesolitica/mallam-1.1B-4096\")\n",
        "\n",
        "results = []\n",
        "for sample in test_data:\n",
        "    prompt = f\"Translate to Malay: {sample['input']}\"\n",
        "    output = pipe(prompt, max_length=100, do_sample=False)[0]['generated_text']\n",
        "    results.append({\n",
        "        \"input\": sample[\"input\"],\n",
        "        \"expected\": sample[\"expected\"],\n",
        "        \"output\": output.strip()\n",
        "    })\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T03:09:04.601472Z",
          "iopub.execute_input": "2025-04-20T03:09:04.601856Z",
          "iopub.status.idle": "2025-04-20T03:09:16.109155Z",
          "shell.execute_reply.started": "2025-04-20T03:09:04.601837Z",
          "shell.execute_reply": "2025-04-20T03:09:16.108613Z"
        },
        "id": "a_pDkIiW2A8Y",
        "outputId": "5a0588ad-b1f4-4c39-96ce-6027e00fa3a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Step 3: Evaluate the Output"
      ],
      "metadata": {
        "id": "N6oSaB7t2A8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "import evaluate\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "predictions = [r[\"output\"] for r in results]\n",
        "references = [[r[\"expected\"]] for r in results]  # Note: nested list for BLEU\n",
        "\n",
        "score = bleu.compute(predictions=predictions, references=references)\n",
        "print(f\"BLEU Score: {score['bleu']:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T03:09:16.110434Z",
          "iopub.execute_input": "2025-04-20T03:09:16.110654Z",
          "iopub.status.idle": "2025-04-20T03:09:23.424572Z",
          "shell.execute_reply.started": "2025-04-20T03:09:16.11063Z",
          "shell.execute_reply": "2025-04-20T03:09:23.42387Z"
        },
        "id": "M85fjv2M2A8Z",
        "outputId": "71f1a4d6-3a3f-4e47-f2c9-d4b6c768da02",
        "colab": {
          "referenced_widgets": [
            "34663bf235d448969acea41c63b7513a",
            "5da2121f62164354bad1f708c18046de",
            "35d7dc4f5d12428e90a23a9fea85970a"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34663bf235d448969acea41c63b7513a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5da2121f62164354bad1f708c18046de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35d7dc4f5d12428e90a23a9fea85970a"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "BLEU Score: 0.0000\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for r in results:\n",
        "    print(f\"\\nInput: {r['input']}\")\n",
        "    print(f\"Expected: {r['expected']}\")\n",
        "    print(f\"Model Output: {r['output']}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T03:09:23.425447Z",
          "iopub.execute_input": "2025-04-20T03:09:23.425725Z",
          "iopub.status.idle": "2025-04-20T03:09:23.430246Z",
          "shell.execute_reply.started": "2025-04-20T03:09:23.425684Z",
          "shell.execute_reply": "2025-04-20T03:09:23.429543Z"
        },
        "id": "2KsqTs5W2A8Z",
        "outputId": "d306a875-2843-4a30-bf4a-a45a5e28abc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\nInput: Hello, how are you?\nExpected: Hai, apa khabar?\nModel Output: Translate to Malay: Hello, how are you? I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine. I'm fine.\n\nInput: I love to eat nasi lemak.\nExpected: Saya suka makan nasi lemak.\nModel Output: Translate to Malay: I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat nasi lemak. I love to eat\n\nInput: Where is the nearest hospital?\nExpected: Di manakah hospital yang terdekat?\nModel Output: Translate to Malay: Where is the nearest hospital?\\nPosted by Mohd.Noor Azam bin Atan at 10:00 AM No comments:\\nPosted by Mohd.Noor Azam bin Atan at 10:00 AM No comments:\\nPosted by Mohd.Noor Azam bin Atan at 10:00 AM No comments:\\nPosted by Mohd.Noor Azam bin Atan at 10:00 AM No comments:\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-by-Step: Access Layers, Attention, and Model Internals"
      ],
      "metadata": {
        "id": "6UYCQWdK2A8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model manually (not with pipeline)"
      ],
      "metadata": {
        "id": "BK2PzVXr2A8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect Model Architecture Without Weights"
      ],
      "metadata": {
        "id": "Uw4gzBjp2A8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"mesolitica/mallam-1.1B-4096\")\n",
        "\n",
        "print(config)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "V3LyMhUL2A8a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stack SAT on Mallam"
      ],
      "metadata": {
        "id": "CAdXrHRT2A8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "# Load Mallam model config only (no weights)\n",
        "config = AutoConfig.from_pretrained(\"mesolitica/mallam-1.1B-4096\")\n",
        "mallam_base = AutoModelForCausalLM.from_config(config)  # No pre-trained weights loaded\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T09:07:50.057048Z",
          "iopub.execute_input": "2025-04-20T09:07:50.057632Z",
          "iopub.status.idle": "2025-04-20T09:08:38.271161Z",
          "shell.execute_reply.started": "2025-04-20T09:07:50.057612Z",
          "shell.execute_reply": "2025-04-20T09:08:38.270602Z"
        },
        "id": "RwzjiD9l2A8a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STA Structure"
      ],
      "metadata": {
        "id": "mUY-ZaMW2A8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SyntaxAwareAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(hidden_size, hidden_size)\n",
        "        self.key = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value = nn.Linear(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, hidden_states, syntax_mask=None):\n",
        "        # 🔥 Convert hidden states to float32 to avoid dtype mismatch\n",
        "        hidden_states = hidden_states.to(torch.float32)\n",
        "\n",
        "        Q = self.query(hidden_states)\n",
        "        K = self.key(hidden_states)\n",
        "        V = self.value(hidden_states)\n",
        "\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)\n",
        "\n",
        "        if syntax_mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(syntax_mask == 0, -1e9)\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        context = torch.matmul(attn_probs, V)\n",
        "        output = self.out(context)\n",
        "        return output\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T09:08:45.717673Z",
          "iopub.execute_input": "2025-04-20T09:08:45.718313Z",
          "iopub.status.idle": "2025-04-20T09:08:45.724355Z",
          "shell.execute_reply.started": "2025-04-20T09:08:45.718289Z",
          "shell.execute_reply": "2025-04-20T09:08:45.723592Z"
        },
        "id": "KoAGx2Pm2A8b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrap Everything Into One Model"
      ],
      "metadata": {
        "id": "n57-Jy4f2A8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, input_ids, attention_mask=None, syntax_mask=None, **kwargs):\n",
        "    # Get hidden states from base model\n",
        "    outputs = self.base_model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        output_hidden_states=True,\n",
        "        **kwargs\n",
        "    )\n",
        "    hidden = outputs.hidden_states[-1]  # Shape: [batch, seq_len, hidden_size]\n",
        "\n",
        "    # Apply Syntax-Aware Attention\n",
        "    syntax_out = self.syntax_attn(hidden, syntax_mask)\n",
        "    combined = hidden + syntax_out  # Simple residual connection\n",
        "\n",
        "    return combined  # or return logits if needed\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T09:08:46.346336Z",
          "iopub.execute_input": "2025-04-20T09:08:46.346569Z",
          "iopub.status.idle": "2025-04-20T09:08:46.350683Z",
          "shell.execute_reply.started": "2025-04-20T09:08:46.346552Z",
          "shell.execute_reply": "2025-04-20T09:08:46.350116Z"
        },
        "id": "lwK-C6Lc2A8b"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Initialize Model"
      ],
      "metadata": {
        "id": "y_4dtBtP2A8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stacked model\n",
        "model = MallamWithSAT(mallam_base, hidden_size=config.hidden_size)\n",
        "\n",
        "# Example dummy input\n",
        "batch_size = 2\n",
        "seq_len = 16\n",
        "dummy_input = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n",
        "dummy_mask = torch.ones((batch_size, seq_len), dtype=torch.int64)\n",
        "dummy_syntax_mask = torch.ones((batch_size, seq_len, seq_len))  # You can mask based on syntax tree later\n",
        "\n",
        "# Forward pass (testing architecture)\n",
        "with torch.no_grad():\n",
        "    out = model(input_ids=dummy_input, attention_mask=dummy_mask, syntax_mask=dummy_syntax_mask)\n",
        "    print(out.shape)  # Expect: [batch_size, seq_len, hidden_size]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T09:08:46.700262Z",
          "iopub.execute_input": "2025-04-20T09:08:46.700458Z",
          "iopub.status.idle": "2025-04-20T09:08:48.586723Z",
          "shell.execute_reply.started": "2025-04-20T09:08:46.700443Z",
          "shell.execute_reply": "2025-04-20T09:08:48.585934Z"
        },
        "id": "OS7e_vQk2A8b",
        "outputId": "698fee8f-2d52-4324-c793-faeaefa2b5cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "torch.Size([2, 16, 2048])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Pretrained Weights Later (Optional)"
      ],
      "metadata": {
        "id": "LCssevqb2A8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to load full weights later\n",
        "from transformers import AutoModelForCausalLM\n",
        "mallam_with_weights = AutoModelForCausalLM.from_pretrained(\"mesolitica/mallam-1.1B-4096\")\n",
        "model = MallamWithSAT(mallam_with_weights, hidden_size=config.hidden_size)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T09:18:04.24086Z",
          "iopub.execute_input": "2025-04-20T09:18:04.241709Z",
          "iopub.status.idle": "2025-04-20T09:18:06.611515Z",
          "shell.execute_reply.started": "2025-04-20T09:18:04.241684Z",
          "shell.execute_reply": "2025-04-20T09:18:06.610944Z"
        },
        "id": "AJ1rcdiL2A8c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "-7ZPSD1Z2A8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "def compare_models(base_model, sat_model, input_ids, attention_mask, syntax_mask=None):\n",
        "    base_model.eval()\n",
        "    sat_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Inference with base Mallam\n",
        "        start = time.time()\n",
        "        base_output = base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        base_time = time.time() - start\n",
        "\n",
        "        # Get logits or hidden state depending on model's output\n",
        "        base_hidden = base_output.logits if hasattr(base_output, 'logits') else base_output[0]\n",
        "\n",
        "        # Inference with Mallam + SAT\n",
        "        start = time.time()\n",
        "        sat_output = sat_model(input_ids=input_ids, attention_mask=attention_mask, syntax_mask=syntax_mask)\n",
        "        sat_time = time.time() - start\n",
        "\n",
        "        # Output results\n",
        "        print(f\"[Mallam] Output shape: {base_hidden.shape}, Time: {base_time:.4f}s\")\n",
        "        print(f\"[Mallam + SAT] Output shape: {sat_output.shape}, Time: {sat_time:.4f}s\")\n",
        "\n",
        "        # Optional: calculate diff\n",
        "        diff = torch.abs(base_hidden - sat_output).mean().item()\n",
        "        print(f\"Mean absolute difference in outputs: {diff:.6f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T09:58:59.554193Z",
          "iopub.execute_input": "2025-04-20T09:58:59.554438Z",
          "iopub.status.idle": "2025-04-20T09:58:59.559832Z",
          "shell.execute_reply.started": "2025-04-20T09:58:59.554421Z",
          "shell.execute_reply": "2025-04-20T09:58:59.559294Z"
        },
        "id": "ldIsW1lq2A8c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy inputs\n",
        "batch_size = 2\n",
        "seq_len = 16\n",
        "vocab_size = config.vocab_size  # from your config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dummy_input_ids = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
        "dummy_attention_mask = torch.ones((batch_size, seq_len), dtype=torch.int64).to(device)\n",
        "dummy_syntax_mask = torch.ones((batch_size, seq_len, seq_len)).to(device)  # or a real syntax mask\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T09:59:01.772668Z",
          "iopub.execute_input": "2025-04-20T09:59:01.773336Z",
          "iopub.status.idle": "2025-04-20T09:59:01.778026Z",
          "shell.execute_reply.started": "2025-04-20T09:59:01.773312Z",
          "shell.execute_reply": "2025-04-20T09:59:01.777436Z"
        },
        "id": "Fm6mOVuI2A8c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, input_ids, attention_mask=None, syntax_mask=None):\n",
        "    # Step 1: Get base model hidden states (not logits yet)\n",
        "    base_output = self.base_model.model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "    )\n",
        "    hidden_states = base_output.last_hidden_state  # likely bfloat16\n",
        "\n",
        "    # Step 2: Convert to float32 before passing to SAT\n",
        "    sat_input = hidden_states.to(dtype=torch.float32)\n",
        "\n",
        "    # Step 3: Apply Syntax-Aware Attention\n",
        "    modified_hidden = self.sat(sat_input, syntax_mask=syntax_mask)\n",
        "\n",
        "    # Step 4: Convert back to lm_head's dtype (usually bfloat16)\n",
        "    target_dtype = self.base_model.lm_head.weight.dtype\n",
        "    modified_hidden = modified_hidden.to(dtype=target_dtype)\n",
        "\n",
        "    # Step 5: Get final logits\n",
        "    logits = self.base_model.lm_head(modified_hidden)\n",
        "    return logits\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T10:02:47.52057Z",
          "iopub.execute_input": "2025-04-20T10:02:47.521065Z",
          "iopub.status.idle": "2025-04-20T10:02:47.526276Z",
          "shell.execute_reply.started": "2025-04-20T10:02:47.521038Z",
          "shell.execute_reply": "2025-04-20T10:02:47.525637Z"
        },
        "id": "d6Rya27Y2A8c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "compare_models(mallam_base, model_with_sat, dummy_input_ids, dummy_attention_mask, dummy_syntax_mask)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-20T10:02:49.884487Z",
          "iopub.execute_input": "2025-04-20T10:02:49.884748Z",
          "iopub.status.idle": "2025-04-20T10:02:50.148235Z",
          "shell.execute_reply.started": "2025-04-20T10:02:49.884729Z",
          "shell.execute_reply": "2025-04-20T10:02:50.147304Z"
        },
        "id": "r_g-OAw22A8c",
        "outputId": "f7262b8a-4c50-4915-8da3-602c00a6754f"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31/1151104337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmallam_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_with_sat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_syntax_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_31/325230950.py\u001b[0m in \u001b[0;36mcompare_models\u001b[0;34m(base_model, sat_model, input_ids, attention_mask, syntax_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Inference with Mallam + SAT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msat_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyntax_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msyntax_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0msat_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_31/2372436420.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, syntax_mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Pass through language model head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodified_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16"
          ],
          "ename": "RuntimeError",
          "evalue": "expected mat1 and mat2 to have the same dtype, but got: float != c10::BFloat16",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "nWhKQxJR2A8c"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}